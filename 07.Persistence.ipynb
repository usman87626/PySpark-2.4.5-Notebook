{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Persistence and Reuse\n",
    "RDDs are created and exist predominantly in memory on Executors. By default, RDDs are tran-\n",
    "sient objects that exist only while they are required. After they transform into new RDDs and\n",
    "arenâ€™t needed for any other operations, they are removed permanently. This may be problematic\n",
    "if an RDD is required for more than one action because it must be reevaluated in its entirety each\n",
    "time. An option to address this is to cache or persist the RDD by using the persist() method.\n",
    "Below are the examples with and without persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.189.128:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing SparkContext\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "\n",
    "sc = SparkContext(\"local\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 elements in the collection\n",
      "The first five elements include: [1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "#MULTIPLE OPERATIONS WITHOUT PERSIST METHOD\n",
    "rangeRDD = sc.range(0,100000,1,2)\n",
    "#Filtering only even out of 1 lac entries\n",
    "even  = rangeRDD.filter(lambda x : x%2)\n",
    "#Counting\n",
    "count = even.count()\n",
    "print(f\"There are {count} elements in the collection\")\n",
    "#Action to collect the answer\n",
    "list_of_elements = even.collect()\n",
    "print(f\"The first five elements include: {list_of_elements[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 elements in the collection\n",
      "First Five elements in the collection are: [1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "#WITH PERSISTENCE\n",
    "numbers  = sc.range(0,100000,1,2)\n",
    "evenNum = numbers.filter(lambda x:x%2)\n",
    "evenNum.persist() # instructs Spark to persist evens RDD when the next action requires it\n",
    "#Counting the total\n",
    "counter = evenNum.count()\n",
    "print(f\"There are {counter} elements in the collection\")\n",
    "\n",
    "listElem = even.collect() # does NOT have to recompute the evens RDD\n",
    "print(f\"First Five elements in the collection are: {listElem[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
