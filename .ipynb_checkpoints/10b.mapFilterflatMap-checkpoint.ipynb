{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map():\n",
    "     Syntax: RDD.map(function, preservesPartitioning=False)\n",
    "     \n",
    "The map() transformation is the most basic of all transformations. It evaluates a named or anony-\n",
    "mous function for each element within a dataset partition. One or many map() functions can\n",
    "run asynchronously because they shouldn’t produce any side effects, maintain state, or attempt\n",
    "to communicate or synchronize with other map() operations. That is, they are shared nothing\n",
    "operations.\n",
    "\n",
    "The preservesPartitioning argument is an optional Boolean argument intended for use\n",
    "with RDDs with a partitioner defined—typically a key/value pair RDD (as discussed later in this\n",
    "chapter) in which a key is defined and grouped by a key hash or key range. If this parameter is set\n",
    "to True , the partitions stay intact. This parameter can be used by the Spark scheduler to optimize\n",
    "subsequent operations, such as joins based on the partitioned key.\n",
    "\n",
    "Consider Figure 4.7, where the map() transformation evaluates a function for each input record\n",
    "and emits a transformed output record. In this case, the split function takes a string and produces\n",
    "a list, and each string element in the input data maps to a list element in the output. The result, in\n",
    "this case, is a list of lists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap() :\n",
    "            Syntax: RDD.flatMap(<function>, preservesPartitioning=False)\n",
    "\n",
    "The flatMap() transformation is similar to the map() transformation in that it runs a function\n",
    "against each record in the input dataset. However, flatMap() “flattens” the output, meaning it\n",
    "removes a level of nesting. For example, given a list containing lists of strings, flattening would\n",
    "result in a single list of strings—“flattening” all of the nested lists. Figure 4.8 shows the effect of a\n",
    "flatMap() transformation using the same anonymous ( lambda) function as the map() opera-\n",
    "tion shown in Figure 4.7. Notice that instead of each string producing a respective list object,\n",
    "all elements are flattened into one list. In other words, flatMap() , in this case, produces one\n",
    "combined list as output, in contrast to the list of lists in the map() example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter():\n",
    "                Syntax: RDD.filter(<function>)\n",
    "\n",
    "The filter transformation evaluates a Boolean expression, usually expressed as an anonymous\n",
    "function, against each element in the dataset. The Boolean value returned determines whether\n",
    "the record is included in the resultant output RDD. This is another common transformation used\n",
    "to remove from RDD records that are not required for intermediate processing and that are not\n",
    "included in the final output.\n",
    "\n",
    "Listing 4.13 shows an example of using the map() , flatMap() , and filter() transformations\n",
    "together to convert input text to uppercase. It uses map() and flatMap() to split the text into\n",
    "a combined list of words and then uses filter() to filter the list to return only words that are\n",
    "greater than four characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Copyright',\n",
       " '©',\n",
       " '2015',\n",
       " 'The',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Tennessee.',\n",
       " 'All',\n",
       " 'rights',\n",
       " 'reserved.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licenses = sc.textFile('file:///opt/Spark/licenses')\n",
    "#WINDOWS Users can uncomment the line below\n",
    "# licenses = sc.textFile(\"file:///C:\\\\Spark\\\\licenses\")\n",
    "words = licenses.flatMap(lambda x:x.split(' '))\n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase = words.map(lambda x: x.lower())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['copyright',\n",
       " '©',\n",
       " '2015',\n",
       " 'the',\n",
       " 'university',\n",
       " 'of',\n",
       " 'tennessee.',\n",
       " 'all',\n",
       " 'rights',\n",
       " 'reserved.',\n",
       " '',\n",
       " 'redistribution',\n",
       " 'and',\n",
       " 'use',\n",
       " 'in']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercase.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['redistribution',\n",
       " 'modification,',\n",
       " 'redistributions',\n",
       " 'redistributions',\n",
       " 'documentation',\n",
       " 'distribution.',\n",
       " 'merchantability',\n",
       " 'consequential',\n",
       " 'interruption)',\n",
       " 'documentation']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long = lowercase.filter(lambda x : len(x) > 12)\n",
    "long.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a standard axiom in the world of Big Data programming: “Filter early, filter often.” This\n",
    "refers to the fact that there is no value in carrying records or fields through a process where they\n",
    "are not needed. Both the filter() and map() functions can be used to achieve this objective.\n",
    "That said, in many cases Spark—through its key runtime characteristic of lazy execution—\n",
    "attempts to optimize routines for you even if you do not explicitly do this yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
