{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations on Sets:\n",
    "Set operations are conceptually similar to mathematical set operations. A set function operates\n",
    "against two RDDs and results in one RDD. Consider the Venn diagram shown in Figure below, which\n",
    "shows a set of odd integers and a subset of Fibonacci numbers. The following sections use these\n",
    "two sets to demonstrate the various set transformations available in the Spark API.\n",
    "\n",
    "![Transformations-on-Sets](img/sets-tranformation.png \"Set Transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# union():\n",
    "                        Syntax:   RDD.union(<otherRDD>)\n",
    "The union() transformation takes one RDD and appends another RDD to it, resulting in a\n",
    "combined output RDD. The RDDs are not required to have the same schema or structure. For\n",
    "instance, the first RDD can have five fields, whereas the second can have more or fewer than five\n",
    "fields.\n",
    "\n",
    "The union() transformation does not filter duplicates from the output RDD in the case that two\n",
    "unioned RDDs have records that are identical to each other. To filter duplicates, you could follow\n",
    "the union() transformation with the distinct() function discussed previously.\n",
    "\n",
    "The RDD that results from a union() operation is not sorted either, but you could sort it by\n",
    "following union() with a sortBy() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.id', 'local-1590561599286'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '61336'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.name', 'Transformations-on-Sets'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'DESKTOP-I7971JS')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext,SparkConf\n",
    "\n",
    "configure = SparkConf().setAppName(\"Transformations-on-Sets\").setMaster(\"local\")\n",
    "sc = SparkContext(conf = configure)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Transformations-on-Sets\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 0, 1, 2, 3, 5, 8]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds = sc.parallelize([1,3,5,7,9])\n",
    "fibonacci = sc.parallelize([0,1,2,3,5,8])\n",
    "odds.union(fibonacci).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intersection():\n",
    "                Syntax:  RDD.intersection(<otherRDD>)\n",
    "The intersection() transformation returns elements that are present in both RDDs. In other\n",
    "words, it returns the overlap between two sets. The elements or records must be identical in both\n",
    "sets, with each respective recordâ€™s data structure and all of its fields matching in both RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds.intersection(fibonacci).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subtract():\n",
    "                Syntax:  RDD.subtract(<otherRDD>, numPartitions=None)\n",
    "The subtract() transformation, as shown in Listing 4.43, returns all elements from the first\n",
    "RDD that are not present in the second RDD. This is an implementation of a mathematical set\n",
    "subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds.subtract(fibonacci).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subtractByKey():\n",
    "                        Syntax: RDD.subtractByKey(<otherRDD>, numPartitions=None)\n",
    "The subtractByKey() transformation is a set operation similar to the subtract transformation.\n",
    "The subtractByKey() transformation returns key/value pair elements from an RDD with keys\n",
    "that are not present in key/value pair elements from otherRDD.\n",
    "\n",
    "The numPartitions argument specifies how many output partitions are to be created in the\n",
    "resultant RDD, and it defaults to the configured spark.default.parallelism value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Baumholder', (49.6489, 7.3975)), ('Melbourne', (37.663712, 144.844788))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities1 = sc.parallelize([\n",
    "                         ('Hayward',(37.668819,-122.080795)),\n",
    "                         ('Baumholder',(49.6489,7.3975)),\n",
    "                        ('Alexandria',(38.820450,-77.050552)),\n",
    "                        ('Melbourne', (37.663712,144.844788))]\n",
    ")\n",
    "\n",
    "cities2 = sc.parallelize([\n",
    "                            ('Boulder Creek',(64.0708333,-148.2236111)),\n",
    "                            ('Hayward',(37.668819,-122.080795)),\n",
    "                            ('Alexandria',(38.820450,-77.050552)),\n",
    "                            ('Arlington', (38.878337,-77.100703))]\n",
    ")\n",
    "\n",
    "cities1.subtractByKey(cities2).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
