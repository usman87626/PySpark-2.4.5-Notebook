{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapValues():\n",
    "            Syntax: RDD.mapValues(<function>)\n",
    "The mapValues() transformation passes each value in a key/value pair RDD through a function\n",
    "(a named or anonymous function specified by the <function> argument) without changing the\n",
    "keys. Like its generalized equivalent map() , mapValues() outputs one element for each input\n",
    "element.\n",
    "    \n",
    "The original RDD’s partitioning is not affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMapValues():\n",
    "            Syntax:   RDD.flatMapValues(<function>)\n",
    "The flatMapValues() transformation passes each value in a key/value pair RDD through a function without changing the keys and produces a flattened list. It works exactly like flatMap() ,\n",
    "which we looked at earlier, returning zero to many output elements per input element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much as with mapValues() , with flatMapValues() the original RDD’s partitioning is retained.\n",
    "The easiest way to contrast mapValues() and flatMapValues() is to look at a practical example.\n",
    "Consider a text file containing a city and a pipe-delimited list of temperatures, as shown here:\n",
    "\n",
    "Hayward,71|69|71|71|72\n",
    "\n",
    "Baumholder,46|42|40|37|39\n",
    "\n",
    "Alexandria,50|48|51|53|44\n",
    "\n",
    "Melbourne,88|101|85|77|74\n",
    "\n",
    "Listing 4.29 simulates the loading of this data into an RDD and uses mapValues() to create a list\n",
    "of key/value pair tuples containing the city and a list of temperatures for the city. It shows the use\n",
    "of flatMapValues() with the same function against the same RDD to create tuples containing\n",
    "the city and a number for each temperature recorded for the city.\n",
    "\n",
    "A simple way to describe this is that mapValues() creates one element per city containing the\n",
    "city name and a list of five temperatures for the city, whereas flatMapValues() flattens the lists\n",
    "to create five elements per city with the city name and a temperature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', [71, 69, 71, 71, 72]),\n",
       " ('Baumholder', [46, 42, 40, 37, 39]),\n",
       " ('Alexandria', [50, 48, 51, 53, 44]),\n",
       " ('Melbourne', [88, 101, 85, 77, 74])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locwtemps = sc.parallelize(\n",
    "        ['Hayward,71|69|71|71|72',\n",
    "        'Baumholder,46|42|40|37|39',\n",
    "        'Alexandria,50|48|51|53|44',\n",
    "        'Melbourne,88|101|85|77|74']\n",
    ")\n",
    "kvpairs = locwtemps.map(lambda x: x.split(','))\n",
    "kvpairs.take(4)\n",
    "# returns :\n",
    "# [['Hayward', '71|69|71|71|72'],\n",
    "# ['Baumholder', '46|42|40|37|39'],\n",
    "# ['Alexandria', '50|48|51|53|44'],\n",
    "# ['Melbourne', '88|101|85|77|74']]\n",
    "locwtemplist = kvpairs.mapValues(lambda x: x.split('|')).mapValues(lambda x: [int(s) for s in x])\n",
    "# .mapValues() on the left will result:\n",
    "# [['Hayward', '71,69,'71','71','72'],\n",
    "# ['Baumholder', ''46','42','40','37','39'],\n",
    "# ['Alexandria', '50','48', '51','53','44'],\n",
    "# ['Melbourne', '88','101','85','77','74']]\n",
    "#then the mapValues() on the right will convert the values in to list of numbers(integers)\n",
    "locwtemplist.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', 71),\n",
       " ('Hayward', 69),\n",
       " ('Hayward', 71),\n",
       " ('Hayward', 71),\n",
       " ('Hayward', 72),\n",
       " ('Baumholder', 46),\n",
       " ('Baumholder', 42),\n",
       " ('Baumholder', 40)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locwtemps = kvpairs.flatMapValues(lambda x: x.split('|')).map(lambda x: (x[0], int(x[1])))\n",
    "\n",
    "locwtemps.take(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping, aggregation, and sorting operations are functionally analogous to their more generalized forms discussed earlier in this chapter ( groupBy() and sortBy() ), again with the difference\n",
    "being that these functions operate specifically on RDDs composed of key/value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Be Cautious of the Repartitioning and Shuffling Effects of Some Functions</h3>\n",
    "Be aware that some functions, such as groupByKey() and reduceByKey() , may result in\n",
    "a repartitioning or require shuffling. Shuffling is a relatively expensive operation because it\n",
    "requires the movement of data between Spark Executors, often located on different Worker\n",
    "nodes. These operations are often necessary and unavoidable, but in some cases, by under-\n",
    "standing the planning and execution of an RDD’s lineage, you may be able to optimize these\n",
    "operations. We discuss partitioning in more detail in Chapter 5.\n",
    "\n",
    "# groupByKey():\n",
    "                Syntax: RDD.groupByKey(numPartitions=None, partitionFunc=<hash_fn>)\n",
    "The groupByKey() transformation groups the values for each key in a key/value pair RDD into a\n",
    "single sequence.\n",
    "\n",
    "The numPartitions argument specifies how many partitions—how many groups, that is—to\n",
    "create. The partitions are created using the partitionFunc argument, which defaults to Spark’s\n",
    "built-in hash partitioner. If numPartitions is None , which is the default, then the configured\n",
    "system default number of partitions is used ( spark.default.parallelism ).\n",
    "\n",
    "Consider the output from Listing 4.29. If you want to calculate the average temperature by city,\n",
    "you first need to group all the values together by their city and then compute the averages.\n",
    "Listing 4.30 shows how to use groupByKey() to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', 70.8),\n",
       " ('Baumholder', 40.8),\n",
       " ('Alexandria', 49.2),\n",
       " ('Melbourne', 85.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continued from Listing 4.29\n",
    "grouped = locwtemps.groupByKey()\n",
    "grouped.take(4)\n",
    "#Result:\n",
    "# [\n",
    "#  ('Hayward', <pyspark.resultiterable.ResultIterable at 0x7f602435c190>),\n",
    "#  ('Baumholder', <pyspark.resultiterable.ResultIterable at 0x7f602435c0d0>),\n",
    "#  ('Alexandria', <pyspark.resultiterable.ResultIterable at 0x7f602435c490>),\n",
    "#  ('Melbourne', <pyspark.resultiterable.ResultIterable at 0x7f602435c1d0>)\n",
    "# ]\n",
    "avgtemps = grouped.mapValues(lambda x: sum(x)/len(x))\n",
    "avgtemps.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that groupByKey() returns a resultiterable object for the grouped values. An iterable\n",
    "object in Python is a sequence object that can loop over. Many functions in Python accept iter-\n",
    "ables as input, such as the sum() and len() functions.\n",
    "<h3>Consider Using reduceByKey() or foldByKey() Instead of\n",
    "groupByKey() </h3>\n",
    "If you group values for the purposes of aggregation, such as by using a sum() or count() for\n",
    "each key, then using reduceByKey() or foldByKey() provides much better performance in\n",
    "many cases. This is because the results of the aggregation function are combined before the\n",
    "shuffle, resulting in a reduced amount of data being shuffled.\n",
    "\n",
    "# reduceByKey():\n",
    "            Syntax: RDD.reduceByKey(function, numPartitions=None, partitionFunc=<hash_fn>)\n",
    "The reduceByKey() transformation merges the values for the keys by using an associative func-\n",
    "tion. The reduceByKey() method is called on a dataset of key/value pairs and returns a dataset of\n",
    "key/value pairs, aggregating values for each key. This function is expressed as follows:\n",
    "\n",
    "v n , v n+1 => v result\n",
    "\n",
    "The numPartitions and partitionFunc arguments behave exactly the same as in the group-\n",
    "ByKey() function. The numPartitions value is effectively the number of reduce tasks to execute,\n",
    "and you can increase this to obtain a higher degree of parallelism. The numPartitions value\n",
    "also affects the number of files produced with saveAsTextFile() or other file-producing Spark\n",
    "actions. For instance, numPartitions=2 produces two output files when the RDD saves to disk.\n",
    "Listing 4.31 takes the same input key/value pairs and produces the same results (average tempera-\n",
    "tures per city) as the previous groupByKey() example—but using the reduceByKey() function\n",
    "instead. This method is preferred for reasons we will discuss shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', 70.8),\n",
       " ('Baumholder', 40.8),\n",
       " ('Alexandria', 49.2),\n",
       " ('Melbourne', 85.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continued from Listing 4.29\n",
    "temptups = locwtemps.mapValues(lambda x: (x, 1))\n",
    "# creates tuples (city, (temp, 1))\n",
    "temptups.collect()\n",
    "# RETURNS:\n",
    "# [\n",
    "#  ('Hayward', (71, 1)),\n",
    "#  ('Hayward', (69, 1)),\n",
    "#  ('Hayward', (71, 1)),\n",
    "#  ('Hayward', (71, 1)),\n",
    "#  ('Hayward', (72, 1)),\n",
    "#  ('Baumholder', (46, 1)),\n",
    "#  ('Baumholder', (42, 1)),\n",
    "#  ('Baumholder', (40, 1)),\n",
    "#  ('Baumholder', (37, 1)),\n",
    "#  ('Baumholder', (39, 1)),\n",
    "#  ('Alexandria', (50, 1)),\n",
    "#  ('Alexandria', (48, 1)),\n",
    "#  ('Alexandria', (51, 1)),\n",
    "#  ('Alexandria', (53, 1)),\n",
    "#  ('Alexandria', (44, 1)),\n",
    "#  ('Melbourne', (88, 1)),\n",
    "#  ('Melbourne', (101, 1)),\n",
    "#  ('Melbourne', (85, 1)),\n",
    "#  ('Melbourne', (77, 1)),\n",
    "#  ('Melbourne', (74, 1))\n",
    "# ]\n",
    "inputstoavg = temptups.reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "inputstoavg.collect()\n",
    "# RETURNS:\n",
    "# [\n",
    "#  ('Hayward', (354, 5)),\n",
    "#  ('Baumholder', (204, 5)),\n",
    "#  ('Alexandria', (246, 5)),\n",
    "#  ('Melbourne', (425, 5))\n",
    "# ]\n",
    "# sums temperatures by city\n",
    "averages = inputstoavg.map(lambda x: (x[0], x[1][0]/x[1][1]))\n",
    "# divides the sum of temperatures by key by the number of readings\n",
    "averages.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging is not an associative operation; you can get around this by creating tuples containing\n",
    "the sum total of values for each key and the count for each key—operations that are associative\n",
    "and commutative—and then computing the average as a final step, as shown in Listing 4.31.\n",
    "\n",
    "\n",
    "Note that reduceByKey() is efficient because it combines values locally on each Executor before\n",
    "each of the combined lists sends to a remote Executor or Executors running the final reduce stage.\n",
    "This is a shuffle operation.\n",
    "\n",
    "Because the same associative and commutative function are run on the local Executor or Worker\n",
    "and again on a remote Executor or Executors, taking a sum function, for example, you can think\n",
    "of this as adding a list of sums as opposed to summing a bigger list of individual values. Because\n",
    "there is less data sent in the shuffle phase, reduceByKey() using a sum function generally\n",
    "performs better than groupByKey() followed by a sum() function.\n",
    "\n",
    "# foldByKey():\n",
    "               Syntax: RDD.foldByKey(zeroValue, function, numPartitions=None,partitionFunc=<hash_fn>)\n",
    "The foldByKey() transformation is functionally similar to the fold() action discussed in the\n",
    "previous section. However, foldByKey() is a transformation that works with predefined key/\n",
    "value pair elements (see Listing 4.32). Both foldByKey() and fold() provide a zeroValue argu-\n",
    "ment of the same type to be used if the RDD is empty.\n",
    "\n",
    "The function supplied is in the generalized aggregate function form:\n",
    "\n",
    "v n , v n+1 => v result\n",
    "\n",
    "This is the same generalization used by the reduceByKey() transformation.\n",
    "The numPartitions and the partitionFunc arguments have the same effect as they do with the\n",
    "groupByKey() and reduceByKey() transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', 72), ('Baumholder', 46), ('Alexandria', 53), ('Melbourne', 101)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continued from Listing 4.29\n",
    "locwtemps.collect()\n",
    "# RETURNS:\n",
    "# [\n",
    "#  ('Hayward', 71),\n",
    "#  ('Hayward', 69),\n",
    "#  ('Hayward', 71),\n",
    "#  ('Hayward', 71),\n",
    "#  ('Hayward', 72),\n",
    "#  ('Baumholder', 46),\n",
    "#  ('Baumholder', 42),\n",
    "#  ('Baumholder', 40),\n",
    "#  ('Baumholder', 37),\n",
    "#  ('Baumholder', 39),\n",
    "#  ('Alexandria', 50),\n",
    "#  ('Alexandria', 48),\n",
    "#  ('Alexandria', 51),\n",
    "#  ('Alexandria', 53),\n",
    "#  ('Alexandria', 44),\n",
    "#  ('Melbourne', 88),\n",
    "#  ('Melbourne', 101),\n",
    "#  ('Melbourne', 85),\n",
    "#  ('Melbourne', 77),\n",
    "#  ('Melbourne', 74)\n",
    "# ]\n",
    "maxbycity = locwtemps.foldByKey(0, lambda x, y: x if x > y else y)\n",
    "maxbycity.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sortByKey():\n",
    "        Syntax:RDD.sortByKey(ascending=True, numPartitions=None, keyfunc=function)\n",
    "The sortByKey() transformation sorts a key/value pair RDD by the predefined key. The sort\n",
    "order is dependent on the underlying key object type, where numeric types are sorted numeri-\n",
    "cally and so on. The difference between sort() , discussed earlier, and sortByKey() is that\n",
    "sort() requires you to identify the key by which to sort, whereas sortByKey() is aware of the\n",
    "key already.\n",
    "\n",
    "Keys are sorted in the order provided by the ascending argument, which defaults to True . The\n",
    "numPartitions argument specifies how many resultant partitions to output using a range parti-\n",
    "tioning function. The keyfunc argument is an optional parameter to use if you want to derive a\n",
    "key from passing the predefined key through another function, as in this example:\n",
    "\n",
    "keyfunc=lambda k: k.lower()\n",
    "\n",
    "Listing 4.33 shows the use of the sortByKey() transformation. The first example shows a simple\n",
    "sort based on the key: a string representing the city name, sorted alphabetically. In the second\n",
    "example, the keys and values are inverted to make the temperature the key and then use sort-\n",
    "ByKey() to list the temperatures in descending numeric order, with the highest temperatures first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, 'Melbourne'), (88, 'Melbourne'), (85, 'Melbourne'), (77, 'Melbourne')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continued from Listing 4.29\n",
    "sortedbykey = locwtemps.sortByKey()\n",
    "sortedbykey.take(4)\n",
    "# returns:\n",
    "# [('Alexandria', 50), ('Alexandria', 48), ('Alexandria', 51), ('Alexandria', 53)]\n",
    "sortedbyval = locwtemps.map(lambda x: (x[1],x[0])) \\\n",
    ".sortByKey(ascending=False)\n",
    "sortedbyval.take(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
